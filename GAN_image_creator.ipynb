{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_image_creator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxm+F7B/HZy3P6UoQRuqLq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxcramseyxx/GAN_image_creator/blob/main/GAN_image_creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0THy0GJxZQm"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "#\"\"\"\n",
        "#Created on Tue Apr  6 16:43:14 2021\n",
        "\n",
        "#@author: xtianramses\n",
        "#\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#setting sme of the hyperparameters\n",
        "batchSize = 64\n",
        "imageSize = 64\n",
        "\n",
        "#creating the transformmations\n",
        "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5,  0.5)),])\n",
        "\n",
        "#koading the dataset\n",
        "dataset = dset.CIFAR10(root = '.\\data', download = True, transform = transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers= 0)\n",
        "\n",
        "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "        \n",
        "#Defining the generator\n",
        "class G(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(G, self).__init__()        #super activates inheritance\n",
        "        self.main = nn.Sequential(\n",
        "                nn.ConvTranspose2d(100,512, 4, 1, 0, bias= False), #100=size of input, 512 num of feature maps output, 4=kernal size 4x4, 1 = stride, 0=padding\n",
        "                nn.BatchNorm2d(512), #512= feature maps\n",
        "                nn.ReLU(True), #arg inplace default is False\n",
        "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), #512= inputs from last cell, 256= new outputs  \n",
        "                nn.BatchNorm2d(256), #256= feature maps\n",
        "                nn.ReLU(True), #arg inplace default is False\n",
        "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "                nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, input): #random input vector to generate noise to make a picture\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "    \n",
        "#Creating the Generator\n",
        "netG = G()\n",
        "netG.apply(weights_init)\n",
        "\n",
        "#Defining the Discriminator\n",
        "class D(nn.Module):\n",
        "        \n",
        "    def __init__(self):\n",
        "        super(D, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "                nn.LeakyReLU(0.2, inplace = True), #0.2=neg slope, inplace activates the activation function\n",
        "                nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.LeakyReLU(0.2, inplace =  True),\n",
        "                nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.LeakyReLU(0.2, inplace = True),\n",
        "                nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.Sigmoid()    \n",
        "        )\n",
        "        \n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1) #.view(-1) flatens the convolution\n",
        "        \n",
        "#Creating the Discriminator\n",
        "netD = D()\n",
        "netD.apply(weights_init)\n",
        "    \n",
        "#Training he DCGANs\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))  \n",
        "\n",
        "for epoch in range(25):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      \n",
        "        #1st step: Updating the weights of the Discriminator\n",
        "        \n",
        "        netD.zero_grad()\n",
        "        \n",
        "        #Training the Discriminator with a real image of the dataset\n",
        "        \n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(input)\n",
        "        errD_real  = criterion(output, target)\n",
        "        \n",
        "        #Training the Discriminator with a fake image generated by the generator\n",
        "        \n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) #100 = feature maps, 1 ,1 = of 1x1\n",
        "        fake = netG(noise)\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        output = netD(fake.detatch()) #won't be a part of the SGD\n",
        "        errD_fake  = criterion(output, target)\n",
        "        \n",
        "        #Backpropagating this total error\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "    \n",
        "        #2nd step: Updating the weightsf the neural network of the generator\n",
        "        \n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        #3rd step: rinting the losses nd saving the real images and the generated images\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0])) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).\n",
        "        if i % 100 == 0: # Every 100 steps:\n",
        "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True) # We save the real images of the minibatch.\n",
        "            fake = netG(noise) # We get our fake generated images.\n",
        "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True) # We also save the fake generated images of the minibatch.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}